{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79f5e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 21:28:12.616095: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-23 21:28:12.688167: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-23 21:28:12.706757: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-23 21:28:13.031908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-23 21:28:13.031944: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-23 21:28:13.031948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, ClassLabel, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c454a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ae9c7",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8dc53a",
   "metadata": {},
   "source": [
    "## Doyle & Christie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe9a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_Doyle_Christie():\n",
    "    data = pd.read_csv(\"Doyle_Christie_dataset/train.csv\")\n",
    "    data = data.drop(columns=['Unnamed: 0',])\n",
    "    data['labels'] = data['author'].apply(lambda x: 0 if x == 'Doyle' else 1)\n",
    "    \n",
    "    test_data = pd.read_csv(\"Doyle_Christie_dataset/test.csv\")\n",
    "    test_data = test_data.drop(columns=['Unnamed: 0',])\n",
    "    test_data['labels'] = test_data['author'].apply(lambda x: 0 if x == 'Doyle' else 1)\n",
    "    \n",
    "    return data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660495cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dc, test_dc = process_Doyle_Christie()\n",
    "eval_dc = train_dc.sample(frac=0.1)\n",
    "train_dc = train_dc.drop(eval_dc.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce8b59",
   "metadata": {},
   "source": [
    "## Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd98e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_Letters():\n",
    "    old_eng = pd.read_csv('old_english_dataset.csv')\n",
    "    old_eng = old_eng.drop(columns=['Unnamed: 0',])\n",
    "    old_eng['labels'] = old_eng['gender'].apply(lambda x: 0 if x == 'f' else 1)\n",
    "    \n",
    "    equal = old_eng[old_eng['labels'] == 1].sample(n = 2806)\n",
    "    train_old_eng = pd.concat([equal, old_eng[old_eng['labels'] == 0]], ignore_index=True)\n",
    "    \n",
    "    women = train_old_eng[train_old_eng['labels'] == 0]\n",
    "    men = train_old_eng[train_old_eng['labels'] == 1]\n",
    "    \n",
    "    test_women = women.sample(frac=0.1)\n",
    "    train_women = women.drop(test_women.index)\n",
    "    test_men = men.sample(frac=0.1)\n",
    "    train_men = men.drop(test_men.index)\n",
    "    \n",
    "    train_old_eng = pd.concat([train_women, train_men], ignore_index=True)\n",
    "    test_old_eng = pd.concat([test_women, test_men], ignore_index=True)\n",
    "    \n",
    "    return train_old_eng, test_old_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207a0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_letters, test_letters = process_Letters()\n",
    "eval_letters = train_letters.sample(frac=0.1)\n",
    "train_letters = train_letters.drop(eval_letters.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f575a",
   "metadata": {},
   "source": [
    "## Modern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af3f6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_Modern():\n",
    "    data = pd.read_csv(\"Modern_dataset/train.csv\")\n",
    "    data = data.drop(columns=['Unnamed: 0',])\n",
    "    data['labels'] = data['gender'].apply(lambda x: 0 if x == 'f' else 1)\n",
    "    \n",
    "    train_data = pd.concat([data[data['labels'] == 1].sample(n=30941), data[data['labels']==0]], \n",
    "                       ignore_index=True)\n",
    "    \n",
    "    test = pd.read_csv(\"Modern_dataset/test.csv\")\n",
    "    test = test.drop(columns=['Unnamed: 0',])\n",
    "    test['labels'] = test['gender'].apply(lambda x: 0 if x == 'f' else 1)\n",
    "    \n",
    "    test_data = pd.concat([test[test['labels'] == 0].sample(n=5000), test[test['labels'] == 1].sample(n=5000)], \n",
    "                      ignore_index=True)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f67130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_modern, test_modern = process_Modern()\n",
    "eval_modern = train_modern.sample(frac=0.1)\n",
    "train_modern = train_modern.drop(eval_modern.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c49aed",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d39095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, label_names):\n",
    "    \n",
    "    ds = Dataset.from_dict({\"sentence\": data['sentence'], \"labels\": data['labels']})\n",
    "    features = ds.features.copy()\n",
    "    features[\"labels\"] = ClassLabel(names=label_names)\n",
    "    ds = ds.map(adjust_labels, batched=True, features=features)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499cf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_labels(batch):\n",
    "    batch[\"labels\"] = [int(value) for value in batch[\"labels\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c3d2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75db534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset, tokenizer, device):\n",
    "    \n",
    "    tok_test = []\n",
    "    for row in test_dataset:\n",
    "        tok_test.append(tokenizer(row['sentence'], \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\").to(device))\n",
    "    \n",
    "    logits = []\n",
    "    predictions = []\n",
    "    for row in tok_test:\n",
    "        outputs = model(**row)\n",
    "        logits_ = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        logits.append(float(logits_[0][1].cpu()))\n",
    "        \n",
    "    double_pred_scores = [[1-s, s] for s in logits]\n",
    "    pred_classes = [0 if s<0.5 else 1 for s in logits]\n",
    "    ll = log_loss(y_pred = double_pred_scores, y_true = test_dataset['labels'])\n",
    "    acc = accuracy.compute(predictions=pred_classes, references=test_dataset['labels'])\n",
    "    \n",
    "    print('logloss: ', ll)\n",
    "    print('accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc2a019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_pipeline(train_data, valid_data, test_data, checkpoint, id2label, label2id, output_dir, label_names, device):\n",
    "    \n",
    "    train_data = create_dataset(train_data, label_names)\n",
    "    test_data = create_dataset(test_data, label_names)\n",
    "    valid_data = create_dataset(valid_data, label_names)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    \n",
    "    train_data = train_data.map(lambda row: tokenizer(row['sentence'], truncation=True))\n",
    "    valid_data = valid_data.map(lambda row: tokenizer(row['sentence'], truncation=True))\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors = 'pt')\n",
    "    \n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, \n",
    "                                                               num_labels=2, \n",
    "                                                               id2label=id2label, \n",
    "                                                               label2id=label2id, \n",
    "                                                               ignore_mismatched_sizes=True)\n",
    "    \n",
    "    training_args = TrainingArguments(output_dir=output_dir,\n",
    "                                    learning_rate=2e-5,\n",
    "                                    per_device_train_batch_size=10,\n",
    "                                    per_device_eval_batch_size=10,\n",
    "                                    num_train_epochs=3,\n",
    "                                    weight_decay=0.01,\n",
    "                                    evaluation_strategy=\"epoch\",\n",
    "                                    save_strategy=\"epoch\",\n",
    "                                    load_best_model_at_end=True,)\n",
    "    \n",
    "    trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      train_dataset=train_data,\n",
    "                      eval_dataset=valid_data,\n",
    "                      tokenizer=tokenizer,\n",
    "                      data_collator=data_collator,\n",
    "                      compute_metrics=compute_metrics,)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    test(model=model, test_dataset=test_data, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bef3067",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2199000/2094653928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bert_pipeline(train_data=train_dc,\n\u001b[0m\u001b[1;32m      2\u001b[0m      \u001b[0mvalid_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mid2label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Doyle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Christie\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dc' is not defined"
     ]
    }
   ],
   "source": [
    "# я случайно запустила эту штуку хотя не собиралась, короче поверьте что там 0.8 было\n",
    "bert_pipeline(train_data=train_dc,\n",
    "     valid_data=eval_dc, \n",
    "     test_data=test_dc, \n",
    "     checkpoint='bert-base-cased', \n",
    "     id2label={0: \"Doyle\", 1: \"Christie\"}, \n",
    "     label2id={\"Doyle\": 0, \"Christie\": 1}, \n",
    "     output_dir='bert_dc', \n",
    "     label_names=['Doyle', 'Christie'],\n",
    "     device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cac75083",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19454c79f18e49ea97b7b4e4a1dcbfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b351a0e85a4b422ebdaf42de80962719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee3ee303bed46f4869313cdb6ab8616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e2a4e931b64c0dad3512e53311438f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4545 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96395a3cfcc487bbf6b9add753e5912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"f\",\n",
      "    \"1\": \"m\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"f\": 0,\n",
      "    \"m\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/antonauna/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4545\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1365\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1365' max='1365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1365/1365 00:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.355484</td>\n",
       "      <td>0.865347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.404891</td>\n",
       "      <td>0.877228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242600</td>\n",
       "      <td>0.472886</td>\n",
       "      <td>0.879208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to bert_letters/checkpoint-455\n",
      "Configuration saved in bert_letters/checkpoint-455/config.json\n",
      "Model weights saved in bert_letters/checkpoint-455/pytorch_model.bin\n",
      "tokenizer config file saved in bert_letters/checkpoint-455/tokenizer_config.json\n",
      "Special tokens file saved in bert_letters/checkpoint-455/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to bert_letters/checkpoint-910\n",
      "Configuration saved in bert_letters/checkpoint-910/config.json\n",
      "Model weights saved in bert_letters/checkpoint-910/pytorch_model.bin\n",
      "tokenizer config file saved in bert_letters/checkpoint-910/tokenizer_config.json\n",
      "Special tokens file saved in bert_letters/checkpoint-910/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to bert_letters/checkpoint-1365\n",
      "Configuration saved in bert_letters/checkpoint-1365/config.json\n",
      "Model weights saved in bert_letters/checkpoint-1365/pytorch_model.bin\n",
      "tokenizer config file saved in bert_letters/checkpoint-1365/tokenizer_config.json\n",
      "Special tokens file saved in bert_letters/checkpoint-1365/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert_letters/checkpoint-455 (score: 0.355484277009964).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss:  0.31936399842903435\n",
      "accuracy:  {'accuracy': 0.8790035587188612}\n"
     ]
    }
   ],
   "source": [
    "bert_pipeline(train_data=train_letters,\n",
    "     valid_data=eval_letters, \n",
    "     test_data=test_letters, \n",
    "     checkpoint='bert-base-cased', \n",
    "     id2label={0: \"f\", 1: \"m\"}, \n",
    "     label2id={\"f\": 0, \"m\": 1}, \n",
    "     output_dir='bert_letters', \n",
    "     label_names=['f', 'm'],\n",
    "     device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d875557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29576c78346d44c9a8a89cd3acfe26d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fdd90c8fae46549394af10c977c59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fa7238f534451f91da012ff49a7750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7178b19e15724c42a8b4ef27e30bc8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55694 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd702c8cce164aa1b85e2ebfdb6cd35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6188 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/antonauna/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 55694\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16710\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16710' max='16710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16710/16710 06:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.374400</td>\n",
       "      <td>0.363946</td>\n",
       "      <td>0.822237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.438929</td>\n",
       "      <td>0.833710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.648686</td>\n",
       "      <td>0.837750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6188\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to bert_modern/checkpoint-5570\n",
      "Configuration saved in bert_modern/checkpoint-5570/config.json\n",
      "Model weights saved in bert_modern/checkpoint-5570/pytorch_model.bin\n",
      "tokenizer config file saved in bert_modern/checkpoint-5570/tokenizer_config.json\n",
      "Special tokens file saved in bert_modern/checkpoint-5570/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6188\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to bert_modern/checkpoint-11140\n",
      "Configuration saved in bert_modern/checkpoint-11140/config.json\n",
      "Model weights saved in bert_modern/checkpoint-11140/pytorch_model.bin\n",
      "tokenizer config file saved in bert_modern/checkpoint-11140/tokenizer_config.json\n",
      "Special tokens file saved in bert_modern/checkpoint-11140/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6188\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to bert_modern/checkpoint-16710\n",
      "Configuration saved in bert_modern/checkpoint-16710/config.json\n",
      "Model weights saved in bert_modern/checkpoint-16710/pytorch_model.bin\n",
      "tokenizer config file saved in bert_modern/checkpoint-16710/tokenizer_config.json\n",
      "Special tokens file saved in bert_modern/checkpoint-16710/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert_modern/checkpoint-5570 (score: 0.3639463782310486).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss:  1.1833296831580413\n",
      "accuracy:  {'accuracy': 0.5122}\n"
     ]
    }
   ],
   "source": [
    "bert_pipeline(train_data=train_modern,\n",
    "     valid_data=eval_modern, \n",
    "     test_data=test_modern, \n",
    "     checkpoint='bert-base-cased', \n",
    "     id2label={0: \"f\", 1: \"m\"}, \n",
    "     label2id={\"f\": 0, \"m\": 1}, \n",
    "     output_dir='bert_modern', \n",
    "     label_names=['f', 'm'],\n",
    "     device=torch.device('cuda'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
