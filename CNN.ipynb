{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0505b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15457527",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449543a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c336958",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2650423",
   "metadata": {},
   "source": [
    "## Doyle & Christie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18aad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_Doyle_Christie():\n",
    "    data = pd.read_csv(\"Doyle_Christie_dataset/train.csv\")\n",
    "    data = data.drop(columns=['Unnamed: 0',])\n",
    "    data['labels'] = data['author'].apply(lambda x: 0 if x == 'Doyle' else 1)\n",
    "    \n",
    "    test_data = pd.read_csv(\"Doyle_Christie_dataset/test.csv\")\n",
    "    test_data = test_data.drop(columns=['Unnamed: 0',])\n",
    "    test_data['labels'] = test_data['author'].apply(lambda x: 0 if x == 'Doyle' else 1)\n",
    "    \n",
    "    return data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82e9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dc, test_dc = process_Doyle_Christie()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a7efb",
   "metadata": {},
   "source": [
    "## Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c363300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_Letters():\n",
    "    old_eng = pd.read_csv('old_english_dataset.csv')\n",
    "    old_eng = old_eng.drop(columns=['Unnamed: 0',])\n",
    "    old_eng['labels'] = old_eng['gender'].apply(lambda x: 0 if x == 'f' else 1)\n",
    "    \n",
    "    equal = old_eng[old_eng['labels'] == 1].sample(n = 2806)\n",
    "    train_old_eng = pd.concat([equal, old_eng[old_eng['labels'] == 0]], ignore_index=True)\n",
    "    \n",
    "    women = train_old_eng[train_old_eng['labels'] == 0]\n",
    "    men = train_old_eng[train_old_eng['labels'] == 1]\n",
    "    \n",
    "    test_women = women.sample(frac=0.1)\n",
    "    train_women = women.drop(test_women.index)\n",
    "    test_men = men.sample(frac=0.1)\n",
    "    train_men = men.drop(test_men.index)\n",
    "    \n",
    "    train_old_eng = pd.concat([train_women, train_men], ignore_index=True)\n",
    "    test_old_eng = pd.concat([test_women, test_men], ignore_index=True)\n",
    "    \n",
    "    return train_old_eng, test_old_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3593bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_letters, test_letters = process_Letters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0a5a9",
   "metadata": {},
   "source": [
    "## Modern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0dc37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_Modern():\n",
    "    data = pd.read_csv(\"Modern_dataset/train.csv\")\n",
    "    data = data.drop(columns=['Unnamed: 0',])\n",
    "    data['labels'] = data['gender'].apply(lambda x: 0 if x == 'f' else 1)\n",
    "    \n",
    "    train_data = pd.concat([data[data['labels'] == 1].sample(n=30941), data[data['labels']==0]], \n",
    "                       ignore_index=True)\n",
    "    \n",
    "    test = pd.read_csv(\"Modern_dataset/test.csv\")\n",
    "    test = test.drop(columns=['Unnamed: 0',])\n",
    "    test['labels'] = test['gender'].apply(lambda x: 0 if x == 'f' else 1)\n",
    "    \n",
    "    test_data = pd.concat([test[test['labels'] == 0].sample(n=5000), test[test['labels'] == 1].sample(n=5000)], \n",
    "                      ignore_index=True)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278e635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_modern, test_modern = process_Modern()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a577a0",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "678a7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# это код из статьи\n",
    "\n",
    "def process_dataset(dataset, model, vocabulary, inverse_vocabulary):\n",
    "    \n",
    "    processed_dataset = []\n",
    "    \n",
    "    for index, row in tqdm(dataset.iterrows()):\n",
    "        sentence = row['sentence']\n",
    "        q2n = []  # q2n -> question numbers representation\n",
    "        for word in sentence.lower().split():\n",
    "\n",
    "            # Check for unwanted words\n",
    "            if word not in model.key_to_index.keys():\n",
    "                continue\n",
    "\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(inverse_vocabulary)\n",
    "                q2n.append(len(inverse_vocabulary))\n",
    "                inverse_vocabulary.append(word)\n",
    "            else:\n",
    "                q2n.append(vocabulary[word])\n",
    "            \n",
    "        processed_dataset.append(torch.tensor(q2n))\n",
    "                    \n",
    "    return vocabulary, inverse_vocabulary, processed_dataset\n",
    "\n",
    "\n",
    "def prepare_embeddings(model, train, valid, test):\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']\n",
    "\n",
    "    # Iterate over the questions only of both training and test datasets\n",
    "    vocabulary, inverse_vocabulary, processed_train = process_dataset(train, model, vocabulary, inverse_vocabulary)\n",
    "    vocabulary, inverse_vocabulary, processed_valid = process_dataset(valid, model, vocabulary, inverse_vocabulary)\n",
    "    vocabulary, inverse_vocabulary, processed_test = process_dataset(test, model, vocabulary, inverse_vocabulary)\n",
    "\n",
    "    embedding_dim = model.vector_size\n",
    "    embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "    embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "    # Build the embedding matrix\n",
    "    for word, index in vocabulary.items():\n",
    "        if word in model.key_to_index.keys():\n",
    "            embeddings[index] = model.word_vec(word)\n",
    "\n",
    "    return embeddings, embedding_dim, processed_train, processed_valid, processed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92c7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# это код из тетрадки Семена Сорокина?\n",
    "\n",
    "def padding(sequence, sequence_length=60):\n",
    "\n",
    "    if len(sequence)< sequence_length:\n",
    "        add_pad = sequence_length - len(sequence)\n",
    "        return torch.cat((sequence, torch.tensor([0,]*add_pad)), 0)\n",
    "    else:\n",
    "        return sequence[:sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a245e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GendersDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.labels = torch.tensor([int(l) for l in labels], dtype=torch.int64)\n",
    "        self.data = [torch.tensor(row, dtype=torch.int64) for row in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        data = self.data[idx]\n",
    "\n",
    "        return label, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556c721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, matrix, embedding_dim, n_filters, filter_sizes, output_dim, dropout_proba):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.Tensor(matrix))\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[0], embedding_dim))\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[1], embedding_dim))\n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[2], embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_proba)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch, sent_len]\n",
    "        embedded = self.embedding(x) # [batch, sent_len, emb_dim]\n",
    "\n",
    "        embedded = embedded.unsqueeze(1) # [batch, 1, sent_len, emb]\n",
    "        \n",
    "        # self.conv_0(embedded).shape # [batch, n_filters, sent_len-1, 1]\n",
    "               \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3)) # [batch, n_filters, sent_len-1]\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3)) # [batch, n_filters, sent_len-2]\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3)) # [batch, n_filters, sent_len-3]\n",
    "            \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2) # [batch, n_filters]\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2) # [batch, n_filters]\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2) # [batch, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1)) # [batch, 3*n_filters]\n",
    "        \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e630fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# это код с ниса Семена Сорокина\n",
    "\n",
    "def train_model(model, criterion, optimizer, epochs, train_loader, valid_loader):\n",
    "    \n",
    "    model = model.to(device)    \n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    losses = []\n",
    "    best_test_loss = 10.\n",
    "\n",
    "    test_f1 = []\n",
    "\n",
    "    for n_epoch in tqdm(range(epochs)):\n",
    "\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for y, data in train_loader:\n",
    "\n",
    "            data = data.to(device)\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "            y = y.to(device) # [batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = torch.squeeze(model(data))\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for y, data in valid_loader:\n",
    "\n",
    "            data = data.to(device)\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "\n",
    "                pred = torch.squeeze(model(data))\n",
    "                pred = pred.cpu()\n",
    "                y = y.cpu()\n",
    "\n",
    "                loss = criterion(pred, y)\n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "        mean_test_loss = np.mean(test_losses)\n",
    "\n",
    "        print()\n",
    "        print('Losses: train - {:.3f}, test - {:.3f}'.format(np.mean(train_losses), mean_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038a2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test_accuracy(model, test_loader):\n",
    "    test_targets = []\n",
    "    test_losses = []\n",
    "    criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "    preds = []\n",
    "\n",
    "    for y, d in test_loader:\n",
    "\n",
    "        d = d.to(device)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred = torch.squeeze(model(d))\n",
    "            pred = pred.cpu()\n",
    "            preds.append(pred)\n",
    "            test_targets.append(y)\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "            test_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "    print('accuracy: ', binary_accuracy(torch.cat(preds), torch.cat(test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ece9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_pipeline(data, test_data, wv):\n",
    "    \n",
    "    # split data into train and valid\n",
    "    valid_data = data.sample(frac = 0.12)\n",
    "    data = data.drop(valid_data.index)\n",
    "    \n",
    "    # embedding\n",
    "    embeddings, embedding_dim, proc_train, proc_valid, proc_test = prepare_embeddings(model=wv, \n",
    "                                                                                     train=data, \n",
    "                                                                                     valid=valid_data,\n",
    "                                                                                     test=test_data)\n",
    "    \n",
    "    # padding\n",
    "    proc_train = [padding(row) for row in proc_train]\n",
    "    proc_valid = [padding(row) for row in proc_valid]\n",
    "    proc_test = [padding(row) for row in proc_test]\n",
    "    \n",
    "    # make dataloaders\n",
    "    train_dataset = GendersDataset(proc_train, data['labels'])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "    valid_dataset = GendersDataset(proc_valid, valid_data['labels'])\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "    test_dataset = GendersDataset(proc_test, test_data['labels'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "    \n",
    "    \n",
    "    # training\n",
    "    model = CNN(embeddings, 300, 13, [2, 3, 4], 1, 0.25)\n",
    "    train_model(model=model, \n",
    "                criterion=torch.nn.BCEWithLogitsLoss(),\n",
    "                optimizer=torch.optim.Adam(params=model.parameters()),\n",
    "                epochs=50,\n",
    "                train_loader=train_loader,\n",
    "                valid_loader=valid_loader)\n",
    "    \n",
    "    # accuracy\n",
    "    test_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dadd9b3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16415fe814a4a409f2883d3e4761cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3db078196a408fb671413c9a3c91f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6e366ad69a4c458f7236464dbd3e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332860/274856007.py:44: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embeddings[index] = model.word_vec(word)\n",
      "/tmp/ipykernel_332860/2426756907.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = [torch.tensor(row, dtype=torch.int64) for row in data]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a1a2b77f6b4e59b8dda51c8f936a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332860/2323773639.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n",
      "/tmp/ipykernel_332860/2323773639.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses: train - 0.484, test - 3.020\n",
      "\n",
      "Losses: train - 0.960, test - 1.244\n",
      "\n",
      "Losses: train - 0.832, test - 0.708\n",
      "\n",
      "Losses: train - 0.637, test - 2.845\n",
      "\n",
      "Losses: train - 0.856, test - 2.111\n",
      "\n",
      "Losses: train - 0.848, test - 1.002\n",
      "\n",
      "Losses: train - 0.694, test - 0.755\n",
      "\n",
      "Losses: train - 0.661, test - 0.791\n",
      "\n",
      "Losses: train - 0.641, test - 1.260\n",
      "\n",
      "Losses: train - 0.702, test - 0.845\n",
      "\n",
      "Losses: train - 0.639, test - 0.770\n",
      "\n",
      "Losses: train - 0.623, test - 0.774\n",
      "\n",
      "Losses: train - 0.616, test - 0.775\n",
      "\n",
      "Losses: train - 0.606, test - 0.923\n",
      "\n",
      "Losses: train - 0.610, test - 0.817\n",
      "\n",
      "Losses: train - 0.623, test - 0.916\n",
      "\n",
      "Losses: train - 0.595, test - 0.769\n",
      "\n",
      "Losses: train - 0.583, test - 0.787\n",
      "\n",
      "Losses: train - 0.619, test - 1.070\n",
      "\n",
      "Losses: train - 0.581, test - 0.794\n",
      "\n",
      "Losses: train - 0.564, test - 0.841\n",
      "\n",
      "Losses: train - 0.551, test - 1.004\n",
      "\n",
      "Losses: train - 0.551, test - 0.848\n",
      "\n",
      "Losses: train - 0.537, test - 0.836\n",
      "\n",
      "Losses: train - 0.533, test - 0.841\n",
      "\n",
      "Losses: train - 0.522, test - 0.872\n",
      "\n",
      "Losses: train - 0.531, test - 0.917\n",
      "\n",
      "Losses: train - 0.511, test - 0.869\n",
      "\n",
      "Losses: train - 0.501, test - 0.927\n",
      "\n",
      "Losses: train - 0.495, test - 0.904\n",
      "\n",
      "Losses: train - 0.487, test - 0.941\n",
      "\n",
      "Losses: train - 0.482, test - 0.909\n",
      "\n",
      "Losses: train - 0.477, test - 0.933\n",
      "\n",
      "Losses: train - 0.469, test - 0.945\n",
      "\n",
      "Losses: train - 0.461, test - 0.953\n",
      "\n",
      "Losses: train - 0.457, test - 0.968\n",
      "\n",
      "Losses: train - 0.447, test - 0.977\n",
      "\n",
      "Losses: train - 0.439, test - 1.018\n",
      "\n",
      "Losses: train - 0.437, test - 1.003\n",
      "\n",
      "Losses: train - 0.428, test - 1.036\n",
      "\n",
      "Losses: train - 0.424, test - 1.034\n",
      "\n",
      "Losses: train - 0.418, test - 1.014\n",
      "\n",
      "Losses: train - 0.412, test - 1.098\n",
      "\n",
      "Losses: train - 0.410, test - 1.043\n",
      "\n",
      "Losses: train - 0.407, test - 1.058\n",
      "\n",
      "Losses: train - 0.399, test - 1.077\n",
      "\n",
      "Losses: train - 0.397, test - 1.077\n",
      "\n",
      "Losses: train - 0.391, test - 1.103\n",
      "\n",
      "Losses: train - 0.386, test - 1.112\n",
      "\n",
      "Losses: train - 0.385, test - 1.110\n",
      "accuracy:  tensor(0.5730)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332860/2416813087.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "CNN_pipeline(data=train_dc, test_data=test_dc, wv=wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7e8d416",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7331d3b144a442c98dc52e3dda895e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e33f61f7484269a0413eb34b0f2ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ac4cec9f5041948a311a6630385583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332860/274856007.py:44: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embeddings[index] = model.word_vec(word)\n",
      "/tmp/ipykernel_332860/2426756907.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = [torch.tensor(row, dtype=torch.int64) for row in data]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d708d6c58ef46168f103c9d65cc9158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332860/2323773639.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n",
      "/tmp/ipykernel_332860/2323773639.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses: train - 0.878, test - 0.692\n",
      "\n",
      "Losses: train - 0.700, test - 0.690\n",
      "\n",
      "Losses: train - 0.696, test - 0.683\n",
      "\n",
      "Losses: train - 0.705, test - 0.677\n",
      "\n",
      "Losses: train - 0.678, test - 0.715\n",
      "\n",
      "Losses: train - 0.728, test - 0.663\n",
      "\n",
      "Losses: train - 0.693, test - 0.648\n",
      "\n",
      "Losses: train - 0.671, test - 0.634\n",
      "\n",
      "Losses: train - 0.660, test - 0.629\n",
      "\n",
      "Losses: train - 0.647, test - 0.625\n",
      "\n",
      "Losses: train - 0.642, test - 0.625\n",
      "\n",
      "Losses: train - 0.633, test - 0.623\n",
      "\n",
      "Losses: train - 0.630, test - 0.609\n",
      "\n",
      "Losses: train - 0.612, test - 0.606\n",
      "\n",
      "Losses: train - 0.608, test - 0.597\n",
      "\n",
      "Losses: train - 0.588, test - 0.595\n",
      "\n",
      "Losses: train - 0.583, test - 0.619\n",
      "\n",
      "Losses: train - 0.584, test - 0.588\n",
      "\n",
      "Losses: train - 0.565, test - 0.581\n",
      "\n",
      "Losses: train - 0.548, test - 0.576\n",
      "\n",
      "Losses: train - 0.536, test - 0.575\n",
      "\n",
      "Losses: train - 0.532, test - 0.574\n",
      "\n",
      "Losses: train - 0.521, test - 0.568\n",
      "\n",
      "Losses: train - 0.508, test - 0.572\n",
      "\n",
      "Losses: train - 0.499, test - 0.570\n",
      "\n",
      "Losses: train - 0.487, test - 0.567\n",
      "\n",
      "Losses: train - 0.477, test - 0.570\n",
      "\n",
      "Losses: train - 0.470, test - 0.569\n",
      "\n",
      "Losses: train - 0.458, test - 0.570\n",
      "\n",
      "Losses: train - 0.454, test - 0.570\n",
      "\n",
      "Losses: train - 0.447, test - 0.568\n",
      "\n",
      "Losses: train - 0.436, test - 0.573\n",
      "\n",
      "Losses: train - 0.429, test - 0.576\n",
      "\n",
      "Losses: train - 0.416, test - 0.574\n",
      "\n",
      "Losses: train - 0.411, test - 0.575\n",
      "\n",
      "Losses: train - 0.407, test - 0.577\n",
      "\n",
      "Losses: train - 0.402, test - 0.572\n",
      "\n",
      "Losses: train - 0.391, test - 0.578\n",
      "\n",
      "Losses: train - 0.384, test - 0.575\n",
      "\n",
      "Losses: train - 0.381, test - 0.580\n",
      "\n",
      "Losses: train - 0.369, test - 0.581\n",
      "\n",
      "Losses: train - 0.362, test - 0.582\n",
      "\n",
      "Losses: train - 0.358, test - 0.581\n",
      "\n",
      "Losses: train - 0.351, test - 0.591\n",
      "\n",
      "Losses: train - 0.346, test - 0.595\n",
      "\n",
      "Losses: train - 0.341, test - 0.596\n",
      "\n",
      "Losses: train - 0.336, test - 0.597\n",
      "\n",
      "Losses: train - 0.332, test - 0.591\n",
      "\n",
      "Losses: train - 0.322, test - 0.610\n",
      "\n",
      "Losses: train - 0.320, test - 0.598\n",
      "accuracy:  tensor(0.7295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332860/2416813087.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "CNN_pipeline(data=train_letters, test_data=test_letters, wv=wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "670f68c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67260965096c4be28c48c8e4ff30a0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090dc2f47936415ba848165011a34bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e139f323382248889392170d0a8606b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2187775/274856007.py:44: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embeddings[index] = model.word_vec(word)\n",
      "/tmp/ipykernel_2187775/2426756907.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = [torch.tensor(row, dtype=torch.int64) for row in data]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc6d1e717364b06877e2b707be2bca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2187775/2323773639.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n",
      "/tmp/ipykernel_2187775/2323773639.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses: train - 0.205, test - 5.804\n",
      "\n",
      "Losses: train - 0.495, test - 6.624\n",
      "\n",
      "Losses: train - 0.515, test - 4.577\n",
      "\n",
      "Losses: train - 0.481, test - 3.858\n",
      "\n",
      "Losses: train - 0.461, test - 2.693\n",
      "\n",
      "Losses: train - 0.429, test - 2.746\n",
      "\n",
      "Losses: train - 0.425, test - 4.291\n",
      "\n",
      "Losses: train - 0.429, test - 3.921\n",
      "\n",
      "Losses: train - 0.402, test - 3.353\n",
      "\n",
      "Losses: train - 0.391, test - 4.085\n",
      "\n",
      "Losses: train - 0.377, test - 3.793\n",
      "\n",
      "Losses: train - 0.457, test - 2.875\n",
      "\n",
      "Losses: train - 0.411, test - 2.539\n",
      "\n",
      "Losses: train - 0.415, test - 3.595\n",
      "\n",
      "Losses: train - 0.373, test - 3.069\n",
      "\n",
      "Losses: train - 0.475, test - 3.246\n",
      "\n",
      "Losses: train - 0.400, test - 2.672\n",
      "\n",
      "Losses: train - 0.334, test - 4.056\n",
      "\n",
      "Losses: train - 0.397, test - 2.790\n",
      "\n",
      "Losses: train - 0.372, test - 3.472\n",
      "\n",
      "Losses: train - 0.361, test - 4.894\n",
      "\n",
      "Losses: train - 0.415, test - 3.114\n",
      "\n",
      "Losses: train - 0.345, test - 2.728\n",
      "\n",
      "Losses: train - 0.347, test - 2.780\n",
      "\n",
      "Losses: train - 0.379, test - 3.329\n",
      "\n",
      "Losses: train - 0.365, test - 2.759\n",
      "\n",
      "Losses: train - 0.342, test - 3.082\n",
      "\n",
      "Losses: train - 0.339, test - 2.600\n",
      "\n",
      "Losses: train - 0.331, test - 3.361\n",
      "\n",
      "Losses: train - 0.360, test - 2.825\n",
      "\n",
      "Losses: train - 0.321, test - 3.576\n",
      "\n",
      "Losses: train - 0.320, test - 4.576\n",
      "\n",
      "Losses: train - 0.372, test - 3.261\n",
      "\n",
      "Losses: train - 0.323, test - 2.972\n",
      "\n",
      "Losses: train - 0.321, test - 2.671\n",
      "\n",
      "Losses: train - 0.313, test - 4.874\n",
      "\n",
      "Losses: train - 0.394, test - 3.907\n",
      "\n",
      "Losses: train - 0.387, test - 3.948\n",
      "\n",
      "Losses: train - 0.306, test - 3.045\n",
      "\n",
      "Losses: train - 0.280, test - 2.942\n",
      "\n",
      "Losses: train - 0.282, test - 3.291\n",
      "\n",
      "Losses: train - 0.354, test - 3.729\n",
      "\n",
      "Losses: train - 0.325, test - 3.426\n",
      "\n",
      "Losses: train - 0.282, test - 2.826\n",
      "\n",
      "Losses: train - 0.282, test - 3.208\n",
      "\n",
      "Losses: train - 0.289, test - 3.216\n",
      "\n",
      "Losses: train - 0.344, test - 3.463\n",
      "\n",
      "Losses: train - 0.277, test - 3.876\n",
      "\n",
      "Losses: train - 0.304, test - 3.465\n",
      "\n",
      "Losses: train - 0.294, test - 3.726\n",
      "accuracy:  tensor(0.4998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2187775/2416813087.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "CNN_pipeline(data=train_modern, test_data=test_modern, wv=wv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
