{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7955ff19",
   "metadata": {},
   "source": [
    "| | bag of words + logreg | tf-idf + logreg | bert | some neural network that is not transformer |\n",
    "|-|-----------------------|-----------------|------|---------------------------------------------|\n",
    "| accuracy | 0.7415 | 0.7455 | 0.8085 | ?|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6c5ae",
   "metadata": {},
   "source": [
    "# достать датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c0d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d184b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76941176",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Doyle_Christie_dataset/train.csv\")\n",
    "data = data.drop(columns=['Unnamed: 0',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81ab703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"Doyle_Christie_dataset/test.csv\")\n",
    "test_data = test_data.drop(columns=['Unnamed: 0',])\n",
    "test_data['labels'] = test_data['author'].apply(lambda x: 0 if x == 'Doyle' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a53635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = data['author'].apply(lambda x: 0 if x == 'Doyle' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b44aee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Text Preprocessing '''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize each word\n",
    "    text =  WordPunctTokenizer().tokenize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2bc0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean'] = data['sentence'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61ecb05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "      <th>labels</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Six horses were yoked to it, whereas the other...</td>\n",
       "      <td>Doyle</td>\n",
       "      <td>0</td>\n",
       "      <td>[six, horses, were, yoked, to, it, ,, whereas,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We had said nothing to him about it, so that h...</td>\n",
       "      <td>Doyle</td>\n",
       "      <td>0</td>\n",
       "      <td>[we, had, said, nothing, to, him, about, it, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You’d have done the same, if you have any manh...</td>\n",
       "      <td>Doyle</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, ’, d, have, done, the, same, ,, if, you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It would be to your advantage as well as mine ...</td>\n",
       "      <td>Doyle</td>\n",
       "      <td>0</td>\n",
       "      <td>[it, would, be, to, your, advantage, as, well,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At any rate, we may take it as a hypothesis an...</td>\n",
       "      <td>Doyle</td>\n",
       "      <td>0</td>\n",
       "      <td>[at, any, rate, ,, we, may, take, it, as, a, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>“The police can’t advertise themselves, worse ...</td>\n",
       "      <td>Christie</td>\n",
       "      <td>1</td>\n",
       "      <td>[“, the, police, can, ’, t, advertise, themsel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17996</th>\n",
       "      <td>He admits to  having abandoned twenty men to t...</td>\n",
       "      <td>Christie</td>\n",
       "      <td>1</td>\n",
       "      <td>[he, admits, to, having, abandoned, twenty, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17997</th>\n",
       "      <td>“Absolutely.</td>\n",
       "      <td>Christie</td>\n",
       "      <td>1</td>\n",
       "      <td>[“, absolutely, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>“I asked him at once about Ralph.</td>\n",
       "      <td>Christie</td>\n",
       "      <td>1</td>\n",
       "      <td>[“, i, asked, him, at, once, about, ralph, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999</th>\n",
       "      <td>Would early in August suit you?</td>\n",
       "      <td>Christie</td>\n",
       "      <td>1</td>\n",
       "      <td>[would, early, in, august, suit, you, ?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence    author  labels  \\\n",
       "0      Six horses were yoked to it, whereas the other...     Doyle       0   \n",
       "1      We had said nothing to him about it, so that h...     Doyle       0   \n",
       "2      You’d have done the same, if you have any manh...     Doyle       0   \n",
       "3      It would be to your advantage as well as mine ...     Doyle       0   \n",
       "4      At any rate, we may take it as a hypothesis an...     Doyle       0   \n",
       "...                                                  ...       ...     ...   \n",
       "17995  “The police can’t advertise themselves, worse ...  Christie       1   \n",
       "17996  He admits to  having abandoned twenty men to t...  Christie       1   \n",
       "17997                                       “Absolutely.  Christie       1   \n",
       "17998                  “I asked him at once about Ralph.  Christie       1   \n",
       "17999                    Would early in August suit you?  Christie       1   \n",
       "\n",
       "                                                   clean  \n",
       "0      [six, horses, were, yoked, to, it, ,, whereas,...  \n",
       "1      [we, had, said, nothing, to, him, about, it, ,...  \n",
       "2      [you, ’, d, have, done, the, same, ,, if, you,...  \n",
       "3      [it, would, be, to, your, advantage, as, well,...  \n",
       "4      [at, any, rate, ,, we, may, take, it, as, a, h...  \n",
       "...                                                  ...  \n",
       "17995  [“, the, police, can, ’, t, advertise, themsel...  \n",
       "17996  [he, admits, to, having, abandoned, twenty, me...  \n",
       "17997                                 [“, absolutely, .]  \n",
       "17998      [“, i, asked, him, at, once, about, ralph, .]  \n",
       "17999           [would, early, in, august, suit, you, ?]  \n",
       "\n",
       "[18000 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3442e15",
   "metadata": {},
   "source": [
    "# классический мл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d510a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df5f6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/applying-text-classification-using-logistic-regression-a-comparison-between-bow-and-tf-idf-1f1ed1b83640\n",
    "\n",
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n",
    "#     model = LogisticRegression(C=_C).fit(X_tr, y_tr)\n",
    "    model = LogisticRegression().fit(X_tr, y_tr)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7c66f",
   "metadata": {},
   "source": [
    "## bow + logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d316918",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vect = CountVectorizer()\n",
    "bow_train = bow_vect.fit_transform(data['sentence'])\n",
    "bow_test = bow_vect.transform(test_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4008c911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score with bag of words features 0.7415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonauna/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "bow_model = simple_logistic_classify(X_tr = bow_train, \n",
    "                                     y_tr = data['labels'],\n",
    "                                     X_test = bow_test, \n",
    "                                     y_test = test_data['labels'], \n",
    "                                     description='bag of words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c14bee",
   "metadata": {},
   "source": [
    "## tfidf + logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbddb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_train = tfidf_vect.fit_transform(data['sentence'])\n",
    "tfidf_test = tfidf_vect.transform(test_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bcd36f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score with tf-idf features 0.7455\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = simple_logistic_classify(X_tr = tfidf_train, \n",
    "                                       y_tr = data['labels'],\n",
    "                                       X_test = tfidf_test,\n",
    "                                       y_test = test_data['labels'], \n",
    "                                       description='tf-idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9609342",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f90162fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b348b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181f5077a5a24d22951dd0f9bbd518c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388fad1d80824792bc2b56e4062ba70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def adjust_labels(batch):\n",
    "    batch[\"labels\"] = [int(value) for value in batch[\"labels\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "ds = Dataset.from_dict({\"sentence\": data['sentence'], \"labels\": data['labels']})\n",
    "features = ds.features.copy()\n",
    "features[\"labels\"] = ClassLabel(names=[\"Doyle\", \"Christie\"])\n",
    "ds = ds.map(adjust_labels, batched=True, features=features)\n",
    "\n",
    "\n",
    "ds_test = Dataset.from_dict({\"sentence\": test_data['sentence'], \"labels\": test_data['labels']})\n",
    "features = ds.features.copy()\n",
    "features[\"labels\"] = ClassLabel(names=[\"Doyle\", \"Christie\"])\n",
    "ds_test = ds_test.map(adjust_labels, batched=True, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66431cc9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c2b04f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    return tokenizer(row['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c29b30eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8e3529bc0d497fb5014db186eb8b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61efcf22f58647f9a62a96b4e80024a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(tokenize)\n",
    "ds_test = ds_test.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "adb86a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors = 'pt')\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8cb727e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"Doyle\", 1: \"Christie\"}\n",
    "label2id = {\"Doyle\": 0, \"Christie\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ecb8f48",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Doyle\",\n",
      "    \"1\": \"Christie\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Christie\": 1,\n",
      "    \"Doyle\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     checkpoint, num_labels=2, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fdc9ace0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Doyle\",\n",
      "    \"1\": \"Christie\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Christie\": 1,\n",
      "    \"Doyle\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/antonauna/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "frozen_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "02d6139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, par in model.base_model.named_parameters():\n",
    "#     if name.startswith('bert.embeddings'):\n",
    "#         par.requires_grad = False\n",
    "#     elif name.startswith('bert.encoder.layer.0'):\n",
    "#         par.requires_grad = False\n",
    "#     elif name.startswith('bert.encoder.layer.1.'):\n",
    "#         par.requires_grad = False\n",
    "#     elif name.startswith('bert.encoder.layer.2'):\n",
    "#         par.requires_grad = False   \n",
    "#     elif name.startswith('bert.encoder.layer.3'):\n",
    "#         par.requires_grad = False\n",
    "#     elif name.startswith('bert.encoder.layer.4'):\n",
    "#         par.requires_grad = False\n",
    "#     elif name.startswith('bert.encoder.layer.5'):\n",
    "#         par.requires_grad = False\n",
    "#     elif name.startswith('bert.encoder.layer.6'):\n",
    "#         par.requires_grad = False\n",
    "#     elif name.startswith('bert.encoder.layer.7'):\n",
    "#         par.requires_grad = False\n",
    "#     elif name.startswith('bert.encoder.layer.8'):\n",
    "#         par.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "39c2cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"frozen_bert\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5d29be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=frozen_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds,\n",
    "    eval_dataset=ds_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "126c0928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/antonauna/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5400' max='5400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5400/5400 02:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.459509</td>\n",
       "      <td>0.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.244000</td>\n",
       "      <td>0.710962</td>\n",
       "      <td>0.798000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>1.063771</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to frozen_bert/checkpoint-1800\n",
      "Configuration saved in frozen_bert/checkpoint-1800/config.json\n",
      "Model weights saved in frozen_bert/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in frozen_bert/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in frozen_bert/checkpoint-1800/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to frozen_bert/checkpoint-3600\n",
      "Configuration saved in frozen_bert/checkpoint-3600/config.json\n",
      "Model weights saved in frozen_bert/checkpoint-3600/pytorch_model.bin\n",
      "tokenizer config file saved in frozen_bert/checkpoint-3600/tokenizer_config.json\n",
      "Special tokens file saved in frozen_bert/checkpoint-3600/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to frozen_bert/checkpoint-5400\n",
      "Configuration saved in frozen_bert/checkpoint-5400/config.json\n",
      "Model weights saved in frozen_bert/checkpoint-5400/pytorch_model.bin\n",
      "tokenizer config file saved in frozen_bert/checkpoint-5400/tokenizer_config.json\n",
      "Special tokens file saved in frozen_bert/checkpoint-5400/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from frozen_bert/checkpoint-1800 (score: 0.4595092236995697).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5400, training_loss=0.2837327512105306, metrics={'train_runtime': 132.3107, 'train_samples_per_second': 408.13, 'train_steps_per_second': 40.813, 'total_flos': 1130843093716800.0, 'train_loss': 0.2837327512105306, 'epoch': 3.0})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
